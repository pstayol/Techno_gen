{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>**WGAN(gradient penalty) Training**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from PIL import ImageFile\n",
    "%matplotlib inline\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 2, affine=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(ndf*2, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(ndf * 8,affine=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "#parameters\n",
    "########################\n",
    "ndf= 64\n",
    "ngf= 64\n",
    "nc= 3\n",
    "ngpu= 1\n",
    "epochs= 5\n",
    "nz= 100\n",
    "lr1 = 0.0002\n",
    "lr2 = 0.0002\n",
    "imsize= 32\n",
    "batch_size= 64\n",
    "wr= 2\n",
    "lgp = 10\n",
    "########################\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "def compute_gradient_penalty(discr, real, fake, device=device):\n",
    "    alpha = torch.randn_like(real).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate disc scores\n",
    "    mixed_scores = discr(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "data_path= \"/path/dataset_or_part_of_dataset\"\n",
    "datas= data.ImageFolder(root=data_path,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(imsize),\n",
    "                               transforms.CenterCrop(imsize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "writer_real = SummaryWriter(f\"logs/real5\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake5\")\n",
    "disc= Discriminator(ngpu).to(device)\n",
    "gen= Generator(ngpu).to(device)\n",
    "disc.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "optm_gen = torch.optim.Adam(gen.parameters(), lr=lr1, betas=(0.5,0.99))\n",
    "optm_disc = torch.optim.Adam(disc.parameters(), lr=lr2, betas=(0.5, 0.99))\n",
    "real_l= 1\n",
    "fake_l= 0\n",
    "gen_loss= []\n",
    "disc_loss= []\n",
    "img_list= []\n",
    "step= 0\n",
    "eval_interval= 10\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "dataloader = torch.utils.data.DataLoader(datas, batch_size=batch_size,shuffle=True, num_workers=wr)\n",
    "disciter=5   \n",
    "#Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for i, real_data in enumerate(dataloader, 0):\n",
    "        # Train Discriminator\n",
    "        disc.zero_grad()\n",
    "        # Train with real data\n",
    "        real = real_data[0].to(device)\n",
    "        batch_size = real.size(0)\n",
    "        for _ in range(disciter):\n",
    "            noise = torch.randn(batch_size, nz, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            disc_real = disc(real).reshape(-1)\n",
    "            disc_fake = disc(fake).reshape(-1)\n",
    "            gradpen = compute_gradient_penalty(disc, real, fake)\n",
    "            loss_disc = -(torch.mean(disc_real) - torch.mean(disc_fake)) + lgp *gradpen\n",
    "            disc.zero_grad()\n",
    "            loss_disc.backward(retain_graph=True)\n",
    "            optm_disc.step()\n",
    "        # Train Generator\n",
    "        gen_fake = disc(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optm_gen.step()\n",
    "        # Record Losses\n",
    "        gen_loss.append(loss_gen.item())\n",
    "        disc_loss.append(loss_disc.item())\n",
    "        if (epoch == epochs-1):\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Batch [{i}/{len(dataloader)}]\"\n",
    "                  f\"Discriminator Loss: {loss_disc.item():.4f}, \"\n",
    "                  f\"Generator Loss: {loss_gen.item():.4f}\")\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                real = real_data[0]\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "        step=step+1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6'>**Visualize Loss**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Wgan Generator and Discriminator Loss During Training 5 epochs and 60k samples\")\n",
    "plt.plot(gen_loss,label=\"G\")\n",
    "plt.plot(disc_loss,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
